1
00:00:00,000 --> 00:00:07,000
Hey Beth!

2
00:00:07,000 --> 00:00:09,000
Hello!

3
00:00:09,000 --> 00:00:18,960
Erin, it is always lovely to see you.

4
00:00:18,960 --> 00:00:20,560
And you as well, my dear.

5
00:00:20,560 --> 00:00:23,040
Erin, I have a confession to make.

6
00:00:23,040 --> 00:00:27,360
We did another episode where we talked about ilastik and how ilastik is wonderful, all

7
00:00:27,360 --> 00:00:30,520
which is true, but we told people a rule.

8
00:00:30,520 --> 00:00:32,520
And we don't usually tell people rules.

9
00:00:32,520 --> 00:00:34,520
Yeah, we don't.

10
00:00:34,520 --> 00:00:36,520
We don't make rules.

11
00:00:36,520 --> 00:00:40,520
I think we give a lot of guidance, but we were like, we don't usually give rules without

12
00:00:40,520 --> 00:00:41,520
explaining them.

13
00:00:41,520 --> 00:00:43,640
And we were like, you should always use the one pixel brush.

14
00:00:43,640 --> 00:00:44,640
Yeah, that's true.

15
00:00:44,640 --> 00:00:45,640
That's true.

16
00:00:45,640 --> 00:00:48,280
We kind of mandate that one without explaining it.

17
00:00:48,280 --> 00:00:49,280
Yeah.

18
00:00:49,280 --> 00:00:51,720
So, I figured we should probably explain it.

19
00:00:51,720 --> 00:00:55,400
That would be very nice of us.

20
00:00:55,400 --> 00:01:01,000
But, because it's not always intuitive in ilastik, you know, they make all those gorgeous

21
00:01:01,000 --> 00:01:02,000
brush sizes.

22
00:01:02,000 --> 00:01:07,160
And there are certainly cases where, especially for electron microscopy imaging, you might

23
00:01:07,160 --> 00:01:11,400
use the bigger brush, but for fluorescence imaging and for simple cases, you always

24
00:01:11,400 --> 00:01:16,240
want to avoid what's called overfitting.

25
00:01:16,240 --> 00:01:20,080
And so I thought I would try to give a little bit of a demonstration today about overfitting.

26
00:01:20,080 --> 00:01:21,080
Delightful.

27
00:01:21,080 --> 00:01:22,080
All right.

28
00:01:22,080 --> 00:01:24,680
Let's demonstrate this problem.

29
00:01:25,680 --> 00:01:26,840
All right.

30
00:01:26,840 --> 00:01:32,320
So this is actually a Python notebook that I made with a tool called Marimo, which is

31
00:01:32,320 --> 00:01:34,680
one of my new favorite Python tools.

32
00:01:34,680 --> 00:01:36,680
But you don't need to know anything about code to use it.

33
00:01:36,680 --> 00:01:40,040
And we can drop the link in the show notes.

34
00:01:40,040 --> 00:01:45,840
So this is using a fun plug-in for Marimo called drawData.

35
00:01:45,840 --> 00:01:49,800
And it's going to let me literally draw data.

36
00:01:49,800 --> 00:01:50,800
Wow.

37
00:01:50,800 --> 00:01:51,800
I think it's fun.

38
00:01:51,800 --> 00:01:52,800
That sounds a little dangerous.

39
00:01:53,420 --> 00:01:56,880
I mean, great for demonstration purposes, but like, wouldn't it be nice if I could just

40
00:01:56,880 --> 00:01:58,380
trust me, guys?

41
00:01:58,380 --> 00:02:00,880
Well, this is what my pull, this is what it looks like.

42
00:02:00,880 --> 00:02:03,160
I just drew it on the graph.

43
00:02:03,160 --> 00:02:05,520
Use your powers for good, not for evil.

44
00:02:05,520 --> 00:02:08,840
If great power comes great responsibility.

45
00:02:08,840 --> 00:02:09,840
Yes.

46
00:02:09,840 --> 00:02:16,040
So what this widget gives us access to is we can have four different colors of data that

47
00:02:16,040 --> 00:02:18,040
we can draw.

48
00:02:19,040 --> 00:02:24,200
And we can, I've added some stuff here, so we can say that we want to split them into

49
00:02:24,200 --> 00:02:27,720
two different classes.

50
00:02:27,720 --> 00:02:34,160
And we can decide which color of dot goes into the two different classes.

51
00:02:34,160 --> 00:02:39,960
Right now, it's actually doing a surprisingly good job at splitting our two different, we

52
00:02:39,960 --> 00:02:44,400
have green and red in class two and blue and orange in class one.

53
00:02:44,400 --> 00:02:46,920
And it says it's actually doing 65% accurate.

54
00:02:46,920 --> 00:02:51,400
I think that's somewhat because we have these blue dots that are out here by themselves

55
00:02:51,400 --> 00:02:54,440
and we have these green dots that are down here.

56
00:02:54,440 --> 00:03:01,160
I suspect if I draw some blue dots on there, I mean, 50-50 accuracy is the worst you can

57
00:03:01,160 --> 00:03:02,160
do, right?

58
00:03:02,160 --> 00:03:03,960
Because then you're just flipping a coin.

59
00:03:03,960 --> 00:03:07,400
So we wouldn't expect this to get down to zero, but I was surprised it was even getting

60
00:03:07,400 --> 00:03:09,600
much above 50%.

61
00:03:09,600 --> 00:03:12,640
So if I put some blue dots here.

62
00:03:12,640 --> 00:03:13,640
Yeah.

63
00:03:13,720 --> 00:03:22,520
We're trending down and down towards, well, no, now it's actually getting better again.

64
00:03:22,520 --> 00:03:28,440
But you can see that the accuracy for individual points and for the individual classes changes

65
00:03:28,440 --> 00:03:32,200
as I draw data.

66
00:03:32,200 --> 00:03:36,440
So now let's reset and then let's talk about overfitting.

67
00:03:36,440 --> 00:03:43,200
So overfitting is just the idea that if we have multiple things within the data, some

68
00:03:43,240 --> 00:03:48,680
of which are differences we care about and some of which are differences that we don't.

69
00:03:48,680 --> 00:03:52,600
With machine learning, and here we're using a kind of machine learning called random forest,

70
00:03:52,600 --> 00:03:59,840
which is based on decision trees, we can't always control how the machine chooses to

71
00:03:59,840 --> 00:04:00,840
learn.

72
00:04:00,840 --> 00:04:05,440
This is going to be even worse when we get to thinking about things like deep learning.

73
00:04:05,440 --> 00:04:12,440
But the amount of data that a machine learning classifier like a random forest, which is what

74
00:04:12,440 --> 00:04:19,080
ilastik uses sees, is going to essentially determine how important it thinks a particular

75
00:04:19,080 --> 00:04:20,320
kind of data is.

76
00:04:20,320 --> 00:04:25,920
It will learn to cheat if some data is common and some data is rare to only predict the

77
00:04:25,920 --> 00:04:31,320
common data, because it can do really well while ignoring the rare data.

78
00:04:31,320 --> 00:04:33,320
Did that make sense?

79
00:04:33,320 --> 00:04:34,320
I think so.

80
00:04:34,320 --> 00:04:40,840
So we're saying that we can't rephrase this to make sure I've understood.

81
00:04:41,240 --> 00:04:50,240
If we have a situation where there's a super common phenotype and more rare phenotype,

82
00:04:50,240 --> 00:04:56,880
if most of the information it has is about that common phenotype, can't tell it, oh,

83
00:04:56,880 --> 00:05:02,000
go pay more attention to the rare one because that's what we care about finding.

84
00:05:02,000 --> 00:05:06,400
It is in control of what it's paying attention to.

85
00:05:06,480 --> 00:05:17,720
So it may then say, hey, if only 1% of the time is this rare object, I can be 99% accurate

86
00:05:17,720 --> 00:05:19,640
by just calling everything normal.

87
00:05:19,640 --> 00:05:22,880
I'm just never going to find that and I'm doing great.

88
00:05:22,880 --> 00:05:23,880
Yeah.

89
00:05:23,880 --> 00:05:28,640
And I think there's a great way to sort of demonstrate that.

90
00:05:28,640 --> 00:05:33,880
So here I'm just going to draw in a few dots of class one.

91
00:05:33,880 --> 00:05:40,840
I'm going to draw now a bunch of dots from class two right on top of it.

92
00:05:40,840 --> 00:05:46,600
And this is saying that this is 99% accurate.

93
00:05:46,600 --> 00:05:55,840
And that's because if it just guesses two all the time, it's 99% accurate because there's

94
00:05:55,840 --> 00:05:59,360
319 green dots and four blue ones.

95
00:06:00,360 --> 00:06:02,800
Well, we can see that down there.

96
00:06:02,800 --> 00:06:08,280
It's saying the per class accuracy for class C is 100%.

97
00:06:08,280 --> 00:06:13,960
So all it's guessing every single green dot correctly, but the per class accuracy for

98
00:06:13,960 --> 00:06:15,120
class A is zero.

99
00:06:15,120 --> 00:06:20,680
So it is literally finding no blue dots.

100
00:06:20,680 --> 00:06:21,680
Love it.

101
00:06:21,680 --> 00:06:22,680
Love it.

102
00:06:22,680 --> 00:06:29,200
Beautiful way to cheat there and not helpful to us at all if we care about those blue dots.

103
00:06:29,200 --> 00:06:39,920
So we can even sort of take this a step further where we have our blue dots and we have our

104
00:06:39,920 --> 00:06:41,240
green dots.

105
00:06:41,240 --> 00:06:47,600
And now these are 100% accurate mostly based on X rather than Y, which makes sense.

106
00:06:47,600 --> 00:06:50,760
I've split these on the X axis versus the Y axis.

107
00:06:50,760 --> 00:06:56,120
If we now take a third thing, B, which should be is part of class one.

108
00:06:56,120 --> 00:07:01,040
So it should go into the class A group.

109
00:07:01,040 --> 00:07:08,480
And I just drop a couple little points here.

110
00:07:08,480 --> 00:07:11,240
And I sort of scatter them equally.

111
00:07:11,240 --> 00:07:16,920
Can see as I sort of make them look more like the class C distribution, there's still a

112
00:07:16,920 --> 00:07:19,200
lot more A's than B's.

113
00:07:19,200 --> 00:07:22,280
A, it's still keeping perfect.

114
00:07:22,280 --> 00:07:28,160
And it's still saying that X is by far the most important thing and just the class B is

115
00:07:28,160 --> 00:07:31,440
getting worse and worse and worse.

116
00:07:31,440 --> 00:07:35,920
And then plateauing down at about that 50-50 coin flip because overall there's way more

117
00:07:35,920 --> 00:07:41,320
data in A than B.

118
00:07:41,320 --> 00:07:46,800
I can make B even worse by just sort of adding more and more and more data to A.

119
00:07:47,800 --> 00:07:52,080
I didn't make it worse.

120
00:07:52,080 --> 00:07:55,520
No, I don't understand.

121
00:07:55,520 --> 00:07:57,320
I think just because now it's...

122
00:07:57,320 --> 00:08:00,880
Yeah, I don't actually understand why.

123
00:08:00,880 --> 00:08:09,600
I think because we've now changed our Y distribution.

124
00:08:09,600 --> 00:08:12,160
But you can see how...

125
00:08:12,160 --> 00:08:17,760
But hopefully you can see is that how much data there is is changing the performance

126
00:08:17,760 --> 00:08:23,840
between our ability to sort of say change between two different classes.

127
00:08:23,840 --> 00:08:27,960
If there's tons of data of one kind and very little data of another kind, it makes a big

128
00:08:27,960 --> 00:08:34,240
difference how accurate and how important that other kind becomes.

129
00:08:34,240 --> 00:08:36,080
So how does this relate to ilastik?

130
00:08:37,000 --> 00:08:43,480
Well, it's super tempting in ilastik to take out that really big brush and to make it so

131
00:08:43,480 --> 00:08:50,240
that you have a really big brush where you're covering lots of pixels of an individual cell.

132
00:08:50,240 --> 00:09:01,080
And if you remember back to our episode on ilastik, you might remember when we drew a

133
00:09:01,080 --> 00:09:05,280
lot in the background and we learned a lot about the background.

134
00:09:05,280 --> 00:09:07,680
The background became more confident.

135
00:09:07,680 --> 00:09:11,800
But the actual thing that we were trying to fix there, the error of the background close

136
00:09:11,800 --> 00:09:16,120
to a cell, didn't change at all because we didn't give it any extra information.

137
00:09:16,120 --> 00:09:18,840
Yeah, that makes sense.

138
00:09:18,840 --> 00:09:25,120
So in that case, because it was such a striking thing, we were then able to fix the background

139
00:09:25,120 --> 00:09:27,200
without too much difficulty.

140
00:09:27,200 --> 00:09:32,120
But what we might have accidentally done was say, hey, this part next to the cell is really

141
00:09:32,120 --> 00:09:34,360
unimportant.

142
00:09:34,360 --> 00:09:39,320
Because ilastik uses a really big random forest, it was able to actually figure out, OK, I

143
00:09:39,320 --> 00:09:44,840
have a bunch of pixels that are sort of like, imagine these blue pixels.

144
00:09:44,840 --> 00:09:51,000
We can even actually sort of draw this a little bit more like a cell where if we have a nice

145
00:09:51,000 --> 00:09:52,200
little brush here.

146
00:09:52,200 --> 00:10:02,120
And now we sort of go around.

147
00:10:02,120 --> 00:10:06,160
And our random forces deep enough because these are really simple to say, all right,

148
00:10:06,160 --> 00:10:12,400
if it's within these two things and not across this other thing, and it's doing mostly X

149
00:10:12,400 --> 00:10:16,440
rather than Y, because we have this sort of blue class here.

150
00:10:16,440 --> 00:10:21,240
But the more pixels that we sort of put here, the more times that when our random force

151
00:10:21,240 --> 00:10:25,120
decision trees are trying to say, is this from class one or class two, it's going to

152
00:10:25,120 --> 00:10:27,880
mostly end up with pixels from B here.

153
00:10:27,880 --> 00:10:36,160
So now we change the feature importance and we change our ability to change if we actually

154
00:10:36,160 --> 00:10:41,800
want in a new image to be more like class A, it's going to be hard for the 50 pixels

155
00:10:41,800 --> 00:10:45,160
in class A to compete with the 350 pixels here.

156
00:10:45,160 --> 00:10:50,200
So we have to add more pixels and more pixels and more pixels and more pixels when we didn't

157
00:10:50,200 --> 00:10:53,120
actually need that many to get started.

158
00:10:53,120 --> 00:10:57,720
And so we just make annotation take way longer and be harder to do and maybe be impossible

159
00:10:57,720 --> 00:11:00,440
to do when we have super rare classes.

160
00:11:00,440 --> 00:11:06,480
Like if there's something where there are only 50 pixels, we can't just add 500 more

161
00:11:06,480 --> 00:11:08,880
because our data is our data.

162
00:11:08,880 --> 00:11:14,040
So if we make it so that those 50 pixels always matter by having fewer annotations rather

163
00:11:14,040 --> 00:11:18,000
than lots and lots of annotations, we make sure that those 50 pixels always count in

164
00:11:18,000 --> 00:11:19,000
the end.

165
00:11:19,000 --> 00:11:22,400
Yeah, that makes so much sense.

166
00:11:22,400 --> 00:11:28,720
I think, the cash, this makes me think of a couple of important points that are, I don't

167
00:11:28,720 --> 00:11:36,400
know, one thing too that we have to think about in general in science, right, is when

168
00:11:36,400 --> 00:11:40,160
looking at your metrics, make sure your metrics are actually reporting what you think it's

169
00:11:40,160 --> 00:11:47,280
reporting, that here we have this separation maybe absolutely lovely and giving you a very

170
00:11:47,280 --> 00:11:49,200
high accuracy number.

171
00:11:49,200 --> 00:11:54,720
But again, it could be not because it's performing well on the thing you care about, it's because

172
00:11:54,720 --> 00:11:57,200
it's performing well on the stuff you don't care about.

173
00:11:57,200 --> 00:12:07,600
And so diving into your per class accuracy rather than your separation accuracy is a super

174
00:12:07,600 --> 00:12:12,560
important and helpful thing to think about when thinking about the metrics for your success

175
00:12:12,560 --> 00:12:16,680
at separating and identifying whatever the objects or classes are that you're trying

176
00:12:16,680 --> 00:12:17,680
to identify.

177
00:12:18,680 --> 00:12:19,680
Yeah.

178
00:12:19,680 --> 00:12:25,400
Now, thinking back to what you said when we were looking at that pipeline where you had

179
00:12:25,400 --> 00:12:29,560
the two different things and you were saying that it was actually the correlation between

180
00:12:29,560 --> 00:12:33,640
the two actually behaved really differently when you masked out things that were supposed

181
00:12:33,640 --> 00:12:36,000
to be similar versus weren't supposed to be similar.

182
00:12:36,000 --> 00:12:41,080
It's not something that you necessarily would have naturally come up with, but there's

183
00:12:41,080 --> 00:12:45,480
always little nuances in what are we actually measuring that make the difference.

184
00:12:46,080 --> 00:12:49,640
And the amount of importance that you give to different parts of the measurement when

185
00:12:49,640 --> 00:12:54,000
we get to classifiers and we get to machine learning and where we don't control who makes

186
00:12:54,000 --> 00:13:01,640
the decision can be really, really the difference between success and failure.

187
00:13:01,640 --> 00:13:03,640
Yeah, absolutely.

188
00:13:05,640 --> 00:13:06,640
All right.

189
00:13:06,640 --> 00:13:11,880
Well, I hope this helped create some intuitions around why it is actually important to use

190
00:13:11,880 --> 00:13:17,240
the number one pixel brush in ilastik as well as just how machine learning works in general.

191
00:13:17,240 --> 00:13:18,240
Absolutely.

192
00:13:18,240 --> 00:13:20,600
This is super helpful.

193
00:13:20,600 --> 00:13:24,240
And I think we'll drop this link in the show notes.

194
00:13:24,240 --> 00:13:30,280
You too can go play with this on your own and explore what happens, develop some of your

195
00:13:30,280 --> 00:13:40,120
own further intuition because this is a great way to help think about this, to help think,

196
00:13:40,120 --> 00:13:41,520
to help practice, to help play.

197
00:13:41,560 --> 00:13:42,560
That's great.

198
00:13:42,560 --> 00:13:45,720
And use your data drawing powers for good, not for evil.

199
00:13:45,720 --> 00:13:46,720
Yes, yes.

200
00:13:46,720 --> 00:13:49,720
Thank you so much.

201
00:13:49,720 --> 00:13:50,720
Thank you.

202
00:13:50,720 --> 00:13:51,720
See you next time.

203
00:13:51,720 --> 00:13:52,720
Talk to you later.

